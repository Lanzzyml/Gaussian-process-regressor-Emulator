{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af9a4b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import os\n",
    "from matplotlib import rc\n",
    "from matplotlib import rcParams\n",
    "\n",
    "\n",
    "# Some lines just to make nice plot fonts\n",
    "rcParams['ps.useafm'] = True\n",
    "rcParams['pdf.use14corefonts'] = True\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 10}\n",
    "plt.rc('font', **font)\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"cm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc2c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For doing Gaussian processes with SK-learn\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "# For doing PCA\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4b43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for reading in training set (predictions, nodes, errors) and trail set (prediction, nodes)\n",
    "class Get_Input:\n",
    "\n",
    "\t\n",
    "\t# --------------------------------------------------------------------- READ IN PARAMS ------------------------------------------------------------------------------\n",
    "\tdef __init__(self, paramfile):\n",
    "\t\tself.paramfile = paramfile\n",
    "\t\tself.paraminput = open(self.paramfile).read()\n",
    "\n",
    "\t# --------------------------------------------------------------------- Training set ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\tdef NumNodes(self):\n",
    "\t\treturn int(self.paraminput.split('NumNodes = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\n",
    "\n",
    "\tdef TrainIDs(self):\n",
    "\t\treturn eval(self.paraminput.split('TrainIDs = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\n",
    "\n",
    "\tdef TrainFile(self):\n",
    "\t\treturn self.paraminput.split('TrainFile = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0]\n",
    "\n",
    "\tdef TrainPredCols(self): \n",
    "\t\ttry:\n",
    "\t\t\treturn eval(self.paraminput.split('TrainPredCols = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\t\n",
    "\t\texcept SyntaxError:\n",
    "\t\t\treturn [0,1]\n",
    "\n",
    "\tdef TrainNodesFile(self):\n",
    "\t\treturn self.paraminput.split('TrainNodesFile = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0]\n",
    "\n",
    "\tdef TrainNodeCols(self): \n",
    "\t\ttry:\n",
    "\t\t\treturn eval(self.paraminput.split('TrainNodeCols = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\t\n",
    "\t\texcept SyntaxError:\n",
    "\t\t\treturn None\n",
    "\n",
    "\tdef Train_Error(self):\n",
    "\t\ttry:\n",
    "\t\t\ta = eval(self.paraminput.split('Train_Error = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\n",
    "\t\t\tif isinstance(a, bool):\n",
    "\t\t\t\treturn a\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn False\n",
    "\t\texcept SyntaxError:\n",
    "\t\t\treturn False\n",
    "\n",
    "\n",
    "\tdef Cov_Error(self):\n",
    "\t\ttry:\n",
    "\t\t\ta = eval(self.paraminput.split('Cov_Error = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\n",
    "\t\t\tif isinstance(a, bool):\n",
    "\t\t\t\treturn a\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn False\n",
    "\t\texcept SyntaxError:\n",
    "\t\t\treturn False\n",
    "\n",
    "\n",
    "\tdef TrainErrFile(self):\n",
    "\t\treturn self.paraminput.split('TrainErrFile = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0]\n",
    "\n",
    "\tdef TrainPredErrCol(self): \n",
    "\t\ttry:\n",
    "\t\t\treturn eval(self.paraminput.split('TrainPredErrCol = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\t\n",
    "\t\texcept SyntaxError:\n",
    "\t\t\treturn 1\n",
    "\n",
    "\tdef alpha(self):\n",
    "\t\ttry:\n",
    "\t\t\treturn float(self.paraminput.split('alpha = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\n",
    "\t\texcept (ValueError, TypeError) as error:\n",
    "\t\t\treturn None\n",
    "\n",
    "\tdef Scale_Nodes(self):\n",
    "\t\treturn eval(self.paraminput.split('Scale_Nodes = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\n",
    "\n",
    "\n",
    "\t# --------------------------------------------------------------------- Trial set ------------------------------------------------------------------------------\n",
    "\tdef Run_Trial(self):\n",
    "\t\ttry:\n",
    "\t\t\ta = eval(self.paraminput.split('Run_Trial = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\t\n",
    "\t\t\tif isinstance(a, bool):\n",
    "\t\t\t\treturn a\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn False\n",
    "\t\texcept SyntaxError:\n",
    "\t\t\treturn False\n",
    "\n",
    "\n",
    "\tdef TrialIDs(self):\n",
    "\t\t#print( self.paraminput.split('TrialIDs = ')[-1].split(' ')[0].split('\\t')[0] )\n",
    "\t\ttry:\n",
    "\t\t\ti = eval(self.paraminput.split('TrialIDs = ')[-1].split(' ')[0].split('\\t')[0])\t\n",
    "\t\texcept SyntaxError:\n",
    "\t\t\ti = None\n",
    "\t\treturn i\n",
    "\n",
    "\tdef TrialFile(self):\n",
    "\t\treturn self.paraminput.split('TrialFile = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0]\n",
    "\n",
    "\tdef TrialPredCols(self): \n",
    "\t\ttry:\n",
    "\t\t\treturn eval(self.paraminput.split('TrialPredCols = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\t\n",
    "\t\texcept SyntaxError:\n",
    "\t\t\treturn [0,1]\n",
    "\n",
    "\tdef TrialNodesFile(self):\n",
    "\t\treturn self.paraminput.split('TrialNodesFile = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0]\n",
    "\t\n",
    "\tdef TrialNodeCols(self): \n",
    "\t\ttry:\n",
    "\t\t\treturn eval(self.paraminput.split('TrialNodeCols = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\t\n",
    "\t\texcept SyntaxError:\n",
    "\t\t\treturn None\n",
    "\n",
    "\tdef savedirectory(self):\n",
    "\t\tdef Make_TF_the_savedir():\n",
    "\t\t\tprint( \"No save directory specified/not correctly specified (should start with / character).\")\n",
    "\t\t\tprint( \"Setting savedirectory up inside trial directory.\")\n",
    "\t\t\tTF = self.paraminput.split('Trialfile = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0]\n",
    "\t\t\tdirectory = os.path.dirname(os.path.abspath(TF))\n",
    "\t\t\tdirectory += '/GPR Predictions/'\n",
    "\t\t\treturn directory\n",
    "\n",
    "\t\tdirectory = self.paraminput.split('savedirectory = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0]\n",
    "\t\tif directory == \"\":\n",
    "\t\t\treturn Make_TF_the_savedir()\n",
    "\t\telif directory[0] != '/':\n",
    "\t\t\treturn Make_TF_the_savedir()\t\n",
    "\t\telse:\n",
    "\t\t\treturn directory\n",
    "\n",
    "\n",
    "\t# --------------------------------------------------------------------- PCA ------------------------------------------------------------------------------\n",
    "\tdef Perform_PCA(self):\n",
    "\t\ttry:\n",
    "\t\t\ta = eval(self.paraminput.split('Perform_PCA = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\t\n",
    "\t\t\tif isinstance(a, bool):\n",
    "\t\t\t\treturn a\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn False\n",
    "\t\texcept SyntaxError:\n",
    "\t\t\treturn False\n",
    "\n",
    "\n",
    "\tdef n_components(self):\n",
    "\t\ttry:\n",
    "\t\t\ta = int(self.paraminput.split('n_components = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\n",
    "\t\texcept SyntaxError:\n",
    "\t\t\ta = 9\n",
    "\t\treturn a\n",
    "\n",
    "\tdef BFsFile(self):\n",
    "\t\treturn self.paraminput.split('BFsFile = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0]\n",
    "\n",
    "\tdef BFsDataMean(self):\n",
    "\t\treturn self.paraminput.split('BFsDataMean = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0]\n",
    "\n",
    "\t# --------------------------------------------------------------------- Extra choices ----------------------------------------------------------------------\t\n",
    "\tdef Include_x(self):\n",
    "\t\ttry:\n",
    "\t\t\ta = eval(self.paraminput.split('Include_x = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\t\n",
    "\t\t\tif isinstance(a, bool):\n",
    "\t\t\t\treturn a\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn False\n",
    "\t\texcept SyntaxError:\n",
    "\t\t\treturn False\n",
    "\n",
    "\t\n",
    "\tdef MCMC_HPs(self):\n",
    "\t\ttry:\n",
    "\t\t\ta = eval(self.paraminput.split('MCMC_HPs = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\n",
    "\t\t\tif isinstance(a, bool):\n",
    "\t\t\t\treturn a\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn False\n",
    "\t\texcept SyntaxError:\n",
    "\t\t\treturn False\n",
    "\n",
    "\n",
    "\tdef Cross_Val(self):\n",
    "\t\ttry:\n",
    "\t\t\ta = eval(self.paraminput.split('Cross_Val = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\n",
    "\t\t\tif isinstance(a, bool):\n",
    "\t\t\t\treturn a\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn False\n",
    "\t\texcept SyntaxError:\n",
    "\t\t\treturn False\n",
    "\n",
    "\n",
    "\n",
    "\t# --------------------------------------------------------------------- GPR ----------------------------------------------------------------------\n",
    "\tdef NDim(self):\n",
    "\t\treturn int(self.paraminput.split('NDim = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\n",
    "\n",
    "\tdef n_restarts_optimizer(self):\n",
    "\t\ttry:\n",
    "\t\t\ta = eval(self.paraminput.split('n_restarts_optimizer = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\n",
    "\t\t\tif isinstance(a, int):\n",
    "\t\t\t\treturn a\n",
    "\t\t\telif a == None:\n",
    "\t\t\t\treturn a\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn 20\n",
    "\t\texcept SyntaxError:\n",
    "\t\t\treturn 20\n",
    "\t\n",
    "\n",
    "\tdef HPs(self):\n",
    "\t\tHP_File = self.paraminput.split('HP_File = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0]\n",
    "\t\t#print(\"HP_File is\", HP_File )\n",
    "\t\tif HP_File != \"\" and HP_File != '#':\n",
    "\t\t\t# Then read in the values saved to file.\n",
    "\t\t\th = np.loadtxt( HP_File )\n",
    "\n",
    "\t\telse:\n",
    "\t\t\t# Use the HP array if it's saved. If not, just create the default HP array of unity values.\n",
    "\t\t\ttry: \n",
    "\t\t\t\th = eval(self.paraminput.split('HPs = ')[-1].split(' ')[0].split('\\n')[0].split('\\t')[0])\n",
    "\t\t\t\th = np.array(h)\n",
    "\t\t\texcept SyntaxError:\n",
    "\t\t\t\tprint( \"Could not interpret input HP array (if there is whitespace, remove this). Setting starting HPs to unity.\")\n",
    "\t\t\t\tNDim = self.NDim() + 1 \t# Add 1 for amplitude of kernel.\n",
    "\t\t\t\th = np.ones(NDim)\n",
    "\n",
    "\t\treturn h\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# --------------------------------------------------------------------- LOAD TRAINING SET ------------------------------------------------------------------------------\n",
    "\tdef Fix_Single_Elem(self, a):\n",
    "\t\t# These lines avoid an error if a is only 1 element long and you try to take the length of it:\n",
    "\t\ttry: \n",
    "\t\t\ttmp = len(a) \n",
    "\t\texcept TypeError: \n",
    "\t\t\ta = np.array([a]) \n",
    "\t\treturn a\n",
    "\n",
    "\n",
    "\tdef Load_Training_Set(self):\n",
    "\t\tTF = self.TrainFile()\t\t# Training Filename\t(single string)\n",
    "\t\tIDs = self.TrainIDs()\t\t# IDs of training sets (array)\n",
    "\t\tPC = self.TrainPredCols()\t# Which columns of TrainFile to read (array)\n",
    "\n",
    "\t\tC = self.TrainNodeCols()\t# Which columns of TrainNodesFile to read (array/None)\n",
    "\t\tNF = self.TrainNodesFile()\t# File containing nodes for training, one per row.\n",
    "\t\tSN = self.Scale_Nodes()\t\t# Scale training/trial nodes to [0,1] (True/False)\n",
    "\n",
    "\t\tTrain_Nodes = np.loadtxt(NF, usecols=C, unpack=True).transpose()\t\t\t\t\t\t\t\t\t\t\t\t# Coords of the nodes of the training set.\n",
    "\t\tif SN:\t\t\n",
    "\t\t\t# Normalise Train_Nodes to be in range[0,1]\n",
    "\t\t\tfor i in range(0,Train_Nodes.shape[1]):\n",
    "\t\t\t\tTrain_Nodes[:,i] = abs(Train_Nodes[:,i]-Train_Nodes[:,i].min()) / abs(Train_Nodes[:,i].max()-Train_Nodes[:,i].min())\n",
    "\n",
    "\t\tif TF[-4:] == \".npy\":\t\t# The training file is a pickle. \n",
    "\t\t\t#tmp = np.load('%s%s%s' %(TF.split('XXXX')[0],IDs[0],TF.split('XXXX')[1]))[0]\t\n",
    "\t\t\ttmp = np.load(TF)[0] \t\t\t# Assumes x-array is the zero'th dimension.\n",
    "\t\t\tif len(tmp.shape) == 2:\n",
    "\t\t\t\tTrain_x = tmp[0,:]\n",
    "\t\t\telif len(tmp.shape) == 3:\n",
    "\t\t\t\tTrain_x = tmp[0,0,:]\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint( \"Training set contained in input file:\\n %s\\n Has dimensionality > 3. Too many to handle. Change this!\" %TF)\n",
    "\t\t\t\timport sys\t\t\t\t\n",
    "\t\t\t\tsys.exit()\n",
    "\t\t\t#Train_Pred = np.load('%s%s%s' %(TF.split('XXXX')[0],IDs[0],TF.split('XXXX')[1]))[1]\n",
    "\t\t\tTrain_Pred = np.load(TF)[1]\n",
    "\n",
    "\t\telse:\t\t\t\t\t# Read-in the multiple data files containing the training set predictions for each node.\n",
    "\t\t\tTrain_x = np.loadtxt('%s%s%s' %(TF.split('XXXX')[0],IDs[0],TF.split('XXXX')[1]), usecols=(PC[0],), unpack=True) \t# x-coordinate of training set predictions\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# currently assumes all training set predictions\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# defined at same x-coords.\n",
    "\t\t\tTrain_x = self.Fix_Single_Elem(Train_x)\n",
    "\t\t\tTrain_Pred = np.empty([len(IDs), len(Train_x)])\n",
    "\t\t\tfor i in range(len(IDs)):\n",
    "\t\t\t\tTrain_Pred[i,:] = np.loadtxt('%s%s%s' %(TF.split('XXXX')[0],IDs[i],TF.split('XXXX')[1]), usecols=(PC[1],), unpack=True) \n",
    "\n",
    "\t\t\n",
    "\t\t# If there's errors on Training set, load them. Else, set Error to 1e-6 * Train_Pred.\n",
    "\t\tTE = self.Train_Error()\t\t\t# Error on training set? (True/False)\n",
    "\t\tCE = self.Cov_Error()\t\t\t# Covariance matrix input? (True/False)\n",
    "\t\tTEF = self.TrainErrFile()\t\t# Training error filename\t(string)\n",
    "\t\tPEC = self.TrainPredErrCol()\t# Which column of TrainErrFile to read (int)\n",
    "\n",
    "\n",
    "\t\t# Training set errors\n",
    "\t\tif TE:\t\t\t\t\t\t# Read in an error on training set\t\n",
    "\t\t\tTrain_ErrPred = np.zeros_like(Train_Pred)\t\t\n",
    "\t\t\tfor i in range(len(IDs)):\n",
    "\t\t\t\tif 'XXXX' in TEF:\t# Different error for each node. Read them all in.\n",
    "\t\t\t\t\tname = '%s%s%s' %(TEF.split('XXXX')[0],IDs[i],TEF.split('XXXX')[1])\n",
    "\t\t\t\t\tif CE: \t\t\t# Read in a covariance matrix; take sqrt of diag\n",
    "\t\t\t\t\t\tTrain_ErrPred[i,:] = np.sqrt( np.diag( np.load(name) ) )\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tTrain_ErrPred[i,:] = np.loadtxt(name, usecols=(PEC,), unpack=True)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tif CE: \t\t\t# Read in a covariance matrix; take sqrt of diag\n",
    "\t\t\t\t\t\tTrain_ErrPred[i,:] = np.sqrt( np.diag( np.load(TEF) ) )\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tTrain_ErrPred[i,:] = np.loadtxt(TEF, usecols=(PEC,), unpack=True)\t\n",
    "\t\t\t\t\t\n",
    "\t\telse:\t\t\t\t\t\t# Don't read in error on training set\n",
    "\t\t\tTrain_ErrPred = 1.e-5 *  Train_Pred\t\t\n",
    "\n",
    "\n",
    "\t\treturn Train_x, Train_Pred, Train_ErrPred, Train_Nodes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# --------------------------------------------------------------------- LOAD TRIAL SET COORDS ------------------------------------------------------------------------------\n",
    "\tdef Interpolate_Trial_Onto_Train(self, Train_x, Trial_x, Trials):\n",
    "\t\tnew_Trials = np.zeros([ Trials.shape[0], len(Train_x) ])\n",
    "\t\tfor i in range(Trials.shape[0]):\n",
    "\t\t\tnew_Trials[i,:] = np.interp( Train_x, Trial_x, Trials[i,:] )\n",
    "\t\treturn\tnew_Trials\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tdef Load_Trial_Set(self):\n",
    "\n",
    "\t\tTF = self.TrialFile()\t\t# Trial Filename\t(single string)\n",
    "\t\tIDs = self.TrialIDs()\t\t# IDs of training sets (array)\n",
    "\t\tPC = self.TrialPredCols()\t# Which columns of TrialFile to read (array)\n",
    "\n",
    "\t\tC = self.TrialNodeCols()\t# Which columns of TrialNodesFile to read (array/None)\n",
    "\t\tNF = self.TrialNodesFile()\t# File containing nodes for training, one per row.\n",
    "\t\tSN = self.Scale_Nodes()\t\t# Scale training/trial nodes to [0,1] (True/False)\n",
    "\t\t\n",
    "\t\tTrial_Nodes = np.loadtxt(NF, usecols=C, unpack=True)\t\t# Coords of the nodes of the training set.\n",
    "\n",
    "\t\tif TF[-4:] == \".npy\":\t\t# The trial file is a pickle. \t\n",
    "\t\t\ttmp = np.load(TF)[0] \t\t\t# Assumes x-array is the zero'th dimension.\n",
    "\t\t\tif len(tmp.shape) == 2:\n",
    "\t\t\t\tTrial_x = tmp[0,:]\n",
    "\t\t\telif len(tmp.shape) == 3:\n",
    "\t\t\t\tTrial_x = tmp[0,0,:]\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint( \"Trial set contained in input file:\\n %s\\n Has dimensionality > 3. Too many to handle. Change this!\" %TF )\n",
    "\t\t\t\timport sys\t\t\t\t\n",
    "\t\t\t\tsys.exit()\n",
    "\t\t\tTrial_Pred = np.load(TF)[1]\n",
    "\t\t\t\n",
    "\t\t\t# If multiple Trial predictions, need to transpose the Trial_Nodes\n",
    "\t\t\tTrial_Nodes = Trial_Nodes.transpose()\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tif 'XXXX' in TF:\n",
    "\t\t\t\t# If multiple Trial predictions, need to transpose the Trial_Nodes\n",
    "\t\t\t\tTrial_Nodes = Trial_Nodes.transpose()\n",
    "\t\t\t\tTrial_x = np.loadtxt('%s%s%s' %(TF.split('XXXX')[0],IDs[0],TF.split('XXXX')[1]), usecols=(PC[0],), unpack=True) \t# x-coordinate of training set predictions\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# currently assumes all trial set predictions\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# defined at same x-coords.\n",
    "\t\t\t\t# Avoids an error if Trial_x is only 1 element long.\n",
    "\t\t\t\tTrial_x = self.Fix_Single_Elem(Trial_x)\n",
    "\t\t\t\tTrial_Pred = np.empty([len(IDs), len(Trial_x)])\n",
    "\t\t\t\tfor i in range(len(IDs)):\n",
    "\t\t\t\t\tTrial_Pred[i,:] = np.loadtxt('%s%s%s' %(TF.split('XXXX')[0],IDs[i],TF.split('XXXX')[1]), usecols=(PC[1],), unpack=True)\n",
    "\n",
    "\t\t\telif TF == \"\":\n",
    "\t\t\t\t# Do not read in trial predictions as none are specified.\n",
    "\t\t\t\tTrial_Nodes = Trial_Nodes.transpose()\n",
    "\t\t\t\tTrial_x = None\n",
    "\t\t\t\t\n",
    "\t\t\telse:\n",
    "\t\t\t\t# There is one single Trial Pred prediction specified.\n",
    "\t\t\t\tTrial_x, Trial_Pred = np.loadtxt(TF, usecols=PC, unpack=True)\t\n",
    "\t\t\t\tTrial_x = self.Fix_Single_Elem(Trial_x)\t\n",
    "\t\t\t\tTrial_Pred = self.Fix_Single_Elem(Trial_Pred)\t\n",
    "\n",
    "\t\t\tif len(Trial_Nodes.shape)==1:\n",
    "\t\t\t\t# If only one trial node specified, reshape the Trial_Nodes and Trial_Pred arrays\n",
    "\t\t\t\t# to be [1,len(Trial_Nodes)] and [1,len(Trial_Pred)]\n",
    "\t\t\t\tTrial_Nodes = np.reshape( Trial_Nodes, (1,len(Trial_Nodes)) )\n",
    "\t\t\t\tTrial_Pred = np.reshape( Trial_Pred, (1,len(Trial_Pred)) )\n",
    "\n",
    "\t\tif SN:\n",
    "\t\t\t# Scale Trial_Nodes to be in range [0,1] using Training set\n",
    "\t\t\tTrNF = self.TrainNodesFile()\t\t\t\t\t\t\t\t\t\t\t\t# File containing nodes for training, one per row.\n",
    "\t\t\tC = self.TrainNodeCols()\t\t\t\t\t\t\t\t\t\t\t\t\t# Columns of Training Files to read (array/None)\n",
    "\t\t\tTrain_Nodes = np.loadtxt(TrNF, usecols=C, unpack=True).transpose()\t\t\t# Coords of the nodes of the training set.\n",
    "\t\t\t# Normalise Train_Nodes to be in range[0,1]\n",
    "\t\t\tfor i in range(0,Train_Nodes.shape[1]):\n",
    "\t\t\t\tTrial_Nodes[:,i] = abs(Trial_Nodes[:,i]-Train_Nodes[:,i].min()) / abs(Train_Nodes[:,i].max()-Train_Nodes[:,i].min())\n",
    "\n",
    "\n",
    "\t\t# Check if Trial_x and Train_x are different. If they are, interpolate the trial predictions onto the training.\n",
    "\t\tTF_Train = self.TrainFile()\t\t# Training Filename\t(single string)\n",
    "\t\tIDs_Train = self.TrainIDs()\t\t# IDs of training sets (array)\n",
    "\n",
    "\t\t# Check if training files are a pickle...\n",
    "\t\tif TF_Train[-4:] == \".npy\":\t\t# The training file is a pickle. \t\n",
    "\t\t\ttmp = np.load(TF_Train)[0] \t\t\t# Assumes x-array is the zero'th dimension.\n",
    "\t\t\tif len(tmp.shape) == 2:\n",
    "\t\t\t\tTrain_x = tmp[0,:]\n",
    "\t\t\telif len(tmp.shape) == 3:\n",
    "\t\t\t\tTrain_x = tmp[0,0,:]\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint( \"Training set contained in input file:\\n %s\\n Has dimensionality > 3. Too many to handle. Change this!\" %TF )\n",
    "\t\t\t\timport sys\t\t\t\t\n",
    "\t\t\t\tsys.exit()\n",
    "\t\t\tTrain_x = self.Fix_Single_Elem(Train_x) # Avoids an error if Train_x is 1 element long.\n",
    "\n",
    "\t\t# ...if not, read in Train_x as first column of first Train prediction file\n",
    "\t\telse:\t\t\t\t\t\n",
    "\t\t\tTrain_x = np.loadtxt('%s%s%s' %(TF_Train.split('XXXX')[0],IDs_Train[0],TF_Train.split('XXXX')[1]), usecols=(PC[0],), unpack=True) \t# x-coordinate of training set predictions\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# currently assumes all training set predictions\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# defined at same x-coords.\n",
    "\t\t\tTrain_x = self.Fix_Single_Elem(Train_x) # Avoids an error if Train_x is 1 element long.\n",
    "\n",
    "\t\tif TF != \"\":\n",
    "\t\t\tif np.array_equal(Train_x, Trial_x) == False:\n",
    "\t\t\t\tprint( \"Interpolating the trial predictions onto the x-array of the training predictions.\" )\n",
    "\t\t\t\tTrial_Pred = self.Interpolate_Trial_Onto_Train(Train_x, Trial_x, Trial_Pred)\n",
    "\t\t\t\tTrial_x = Train_x\t\t\n",
    "\t\telse:\n",
    "\t\t\t# No TrialFile specified\n",
    "\t\t\tTrial_x = Train_x\n",
    "\t\t\tTrial_Pred = np.ones([ Trial_Nodes.shape[0], len(Train_x) ])\n",
    "\n",
    "\t\treturn Trial_x, Trial_Pred, Trial_Nodes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09c13a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for performing pincipal component analysis up to a specified number of basis functions.\n",
    "# Options are to find the basis functions for some input data, \n",
    "# or to use pre-specified basis functions to some data to find corresponding weights\n",
    "class PCA_Class:\n",
    "\n",
    "\tdef __init__(self, n_components):\n",
    "\t\tself.n_components = n_components\n",
    "\n",
    "\tdef Load_BFs(self, BFsFile, BFsDataMean):\t\t# write this\n",
    "\t\treturn #xvalues, BFs\n",
    "\t\n",
    "\tdef PCA_BySKL(self, Data):\t# use scikit-learn\n",
    "\t\tpca = PCA(n_components=self.n_components)\n",
    "\t\tWeights = pca.fit_transform(Data)\t\t\t# Weights of the PCA\n",
    "\t\tRecons = pca.inverse_transform(Weights)\t# The reconstructions\n",
    "\t\tBFs = pca.components_                       # Derived basis functions\n",
    "\t\treturn BFs, Weights, Recons\n",
    "\n",
    "\t# Accept some basis functions, BFs, Data to perform PCA on,\n",
    "    # + the mean (in each bin) of the data for which the BFs were identified,\n",
    "    # and manually do the PC Reconstruction\n",
    "\tdef PCA_ByHand(self, BFs, Data, data_Mean):\n",
    "\t\tData_MinusMean = np.empty_like( Data )\n",
    "\t\tfor i in range(len(Data[0,:])):\n",
    "\t\t\tData_MinusMean[:,i] = Data[:,i] - data_Mean[i]\n",
    "\t\tWeights = np.dot(Data_MinusMean,np.transpose(BFs))\n",
    "        \n",
    "\t\tRecons = np.zeros([ Data.shape[0], BFs.shape[1] ]) \n",
    "\t\tfor j in range(len(Weights[:,0])):\t\t    # Scroll through the the Data\n",
    "\t\t\tfor i in range(len(Weights[0,:])): \t    # scroll through the basis functions\n",
    "\t\t\t\tRecons[j,:] += Weights[j,i]*BFs[i,:]\n",
    "\t\t\tRecons[j,:] += data_Mean\n",
    "\t\treturn Weights, Recons\n",
    "\n",
    "\n",
    "\t# Read in Weights, BFS, and data_Mean to recover original statistic\n",
    "\tdef Convert_PCAWeights_2_Predictions(self, Weights, BFs, Mean):\n",
    "\t\tif len(Weights.shape) == 1:\n",
    "\t\t\tWeights = Weights.reshape(1, len(Weights))\n",
    "\t\tPredictions = np.zeros([ Weights.shape[0], BFs.shape[1] ]) \n",
    "\t\tfor j in range(Weights.shape[0]):\t\t# Scroll through number of predictions one needs to make.\n",
    "\t\t\tPredictions[j,:] += Mean\n",
    "\t\t\tfor i in range(BFs.shape[0]):\t\t# Scroll through BFs adding the correct linear combination to the Mean\n",
    "\t\t\t\tPredictions[j,:] += Weights[j,i] * BFs[i,:]\n",
    "\t\treturn Predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc81d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPR_Emu:\n",
    "\n",
    "\tdef __init__(self, nodes_train, y_train, yerr_train, nodes_trial):\n",
    "\t\tself.nodes_train = nodes_train\n",
    "\t\tself.y_train = y_train\n",
    "\t\tself.yerr_train = yerr_train\n",
    "\t\tself.nodes_trial = nodes_trial\n",
    "\n",
    "\n",
    "\tdef GPR(self, p, iterations):\n",
    "\n",
    "\t\tkernel = np.exp(p[0])*ExpSquaredKernel(np.exp(p[1:]), ndim=len(p)-1)\n",
    "\t\tgp = george.GP(kernel)\n",
    "\n",
    "\t\t# Gauge dimensions of y_train, so as to make appropriate output storer\n",
    "\t\t# L is the length of the statistic you're predicting\n",
    "\t\tL = self.y_train.shape[1]\n",
    "\n",
    "\t\t# Gauge size of trial set, so as to make appropriate output storer\n",
    "\t\t# D is the number of nodes.\n",
    "\t\tif len(self.nodes_trial.shape) == 1:\n",
    "\t\t\tD = 1\n",
    "\t\telse:\n",
    "\t\t\tD = self.nodes_trial.shape[0]\n",
    "\n",
    "\n",
    "\t\tGP_OUT = np.zeros([ iterations, D, L ])\t# GP_OUT[i,j,k] = i'th iteration, j'th cosmology, k'th theta bin\n",
    "\n",
    "\t\tfor k in range(L):\n",
    "\t\t\t#print( \"Generating %s predictions for %s trial cosmologies in bin %s\" %(iterations, len(self.nodes_trial), k) )\n",
    "\t\t\tif L == 1:\n",
    "\t\t\t\tgp.compute(self.nodes_train, self.yerr_train)\n",
    "\t\t\telse:\n",
    "\t\t\t\tgp.compute(self.nodes_train, self.yerr_train[:,k])\n",
    "\t\t\tGP_OUT[:,:,k] = gp.sample_conditional(self.y_train[:,k], self.nodes_trial, iterations)\n",
    "\n",
    "\t\t# Now average the iterations and take a standev\n",
    "\t\tGP_AVOUT = np.zeros([D, L])\n",
    "\t\tGP_STDOUT = np.zeros([D, L])\n",
    "\t\tfor j in range(0, D):\n",
    "\t\t\tfor k in range(0, L):\n",
    "\t\t\t\tGP_AVOUT[j,k] = np.mean(GP_OUT[:,j,k])\n",
    "\t\t\t\tGP_STDOUT[j,k] = np.std(GP_OUT[:,j,k])\n",
    "\n",
    "\t\treturn gp, GP_OUT, GP_AVOUT, GP_STDOUT\n",
    "\n",
    "\n",
    "\tdef GPRsk(self, p, alpha, n_restarts_optimizer):\n",
    "\t\t\n",
    "\t\tkernel = np.exp(p[0])*ExpSquaredKernel(np.exp(p[1:]), ndim=len(p)-1)\n",
    "\t\tif n_restarts_optimizer is not None:\n",
    "\t\t\tprint(\"Optimising the emulator with %s restarts...\" % n_restarts_optimizer)\n",
    "\t\t#else:\n",
    "\t\t\t#print(\"Not training the emulator, using pre-set hyperparameters.\")\n",
    "\n",
    "\t\tif alpha is not None:\n",
    "\t\t\tgp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=n_restarts_optimizer, alpha=alpha)\t\t\n",
    "\t\telse:\n",
    "\t\t\tgp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=n_restarts_optimizer)\n",
    "\t\tgp.fit(self.nodes_train, self.y_train)\n",
    "\t\t#print(\"Right, we're running emulator with kernel hyperparameters:\")\n",
    "\t\t#print(gp.kernel_)\n",
    "\t\tGP_AVOUT, GP_STDOUT = gp.predict(self.nodes_trial, return_std=True)\n",
    "\t\t\n",
    "\t\t# -------------------------------------------------------------------------\n",
    "\t\t# THIS BIT OF CODE EXTRACTED MANY SAMPLES FROM THE GP SO THE USER CAN CALC\n",
    "\t\t# THE FULL COVARIANCE OF THE PREDICTIONS. IT IS SLOW AND CAUSES DIMENSIONALITY\n",
    "\t\t# PROBLEMS WHEN RAN WITH Train_Error SET TO True .\n",
    "\n",
    "\t\t#Get_Samples = False\n",
    "\t\t#n_samples = 10\n",
    "\t\t#if Get_Samples:\t\t\n",
    "\t\t\t# Get many samples of the prediction and return these to estimate the full \n",
    "\t\t\t# covariance of the samples.\n",
    "\t\t#\tGP_SAMPLES = gp.sample_y(self.nodes_trial, n_samples=n_samples, random_state=0)\n",
    "\t\t#else:\n",
    "\t\t#\tGP_SAMPLES = np.zeros([ GP_AVOUT.shape[0], GP_AVOUT.shape[1], n_samples ])\n",
    "\n",
    "\t\t#return GP_AVOUT, GP_STDOUT, GP_SAMPLES, self.Process_gpkernel(gp.kernel_)\n",
    "\t\t# -------------------------------------------------------------------------\n",
    "\n",
    "\t\treturn GP_AVOUT, GP_STDOUT, self.Process_gpkernel(gp.kernel_)\n",
    "\n",
    "\tdef Process_gpkernel(self, gpkernel):\n",
    "\t\thp_amp = eval( str(gpkernel).split()[0] )\n",
    "\t\thp_rest = eval( str(gpkernel).split('length_scale=')[-1].split(')')[0] ) \n",
    "\t\treturn np.append( hp_amp, hp_rest ) \n",
    "\n",
    "\tdef Cross_Validation(self, HPs, Perform_PCA, n_components, Train_BFs, Mean_TS, Include_x, x_coord, Train_Error, alpha, n_restarts_optimizer):\n",
    "\t\timport time\n",
    "\t\t# Cycle through training set, omitting one, training on the rest, and testing accuracy with the omitted.\n",
    "\t\tt1 = time.time()\n",
    "\t\tNumNodes = self.y_train.shape[0]\n",
    "\n",
    "\t\tif Perform_PCA:\n",
    "\t\t\tPCACall =  PCA_Class(n_components)\n",
    "\t\t\tif Include_x:\n",
    "\t\t\t\tPredictions = np.empty([NumNodes*len(x_coord),1])\n",
    "\t\t\t\trel_length = n_components\t\t\t\t\t\t\t# the length of the statistic predicted\n",
    "\t\t\telse:\n",
    "\t\t\t\tPredictions = np.empty([NumNodes, len(x_coord)])\n",
    "\t\telse:\n",
    "\t\t\tPredictions = np.empty_like( self.y_train )\n",
    "\t\t\trel_length = len(x_coord)\t\t\t\t\t\t\t\t# the length of the statistic predicted\n",
    "\n",
    "\t\tGP_HPs_AllNodes = np.zeros([ NumNodes, Predictions.shape[1], self.nodes_train.shape[1]+1 ])\n",
    "\t\tfor i in range(NumNodes):\n",
    "\t\n",
    "\t\t\tprint( \"Performing cross-val. for node %s of %s...\" %(i, NumNodes) )\n",
    "\t\t\tif Include_x: \n",
    "\t\t\t\tnew_nodes_trial = self.nodes_train[i*rel_length:(i+1)*rel_length,:]\n",
    "\t\t\t\tnew_y_trial = self.y_train[i*rel_length:(i+1)*rel_length,:]\n",
    "\t\t\t\tnew_nodes_train = np.delete(self.nodes_train, slice(i*rel_length, (i+1)*rel_length), axis=0)\n",
    "\t\t\t\tnew_y_train = np.delete(self.y_train, slice(i*rel_length, (i+1)*rel_length), axis=0)\n",
    "\t\t\t\tnew_yerr_train = np.delete(self.yerr_train, slice(i*rel_length, (i+1)*rel_length), axis=0)\n",
    "\t\t\telse:\n",
    "\t\t\t\tnew_nodes_trial  = self.nodes_train[i,:].reshape((1,len(self.nodes_train[i,:])))\n",
    "\t\t\t\t#print( self.y_train.shape )\n",
    "\t\t\t\tnew_y_trial = self.y_train[i,:]\n",
    "\t\t\t\tnew_nodes_train = np.delete(self.nodes_train, i, axis=0)\n",
    "\t\t\t\tnew_y_train = np.delete(self.y_train, i, axis=0)\n",
    "\t\t\t\tnew_yerr_train = np.delete(self.yerr_train, i, axis=0)\n",
    "\t\n",
    "\t\t\tif Train_Error:\n",
    "\t\t\t\t# If providing an error, SKL requires you run each x-bin separately:\n",
    "\t\t\t\tGP_AVOUT = np.zeros([ new_nodes_trial.shape[0], new_y_train.shape[1] ])\n",
    "\t\t\t\tGP_STDOUT = np.zeros([ new_nodes_trial.shape[0], new_y_train.shape[1] ])\n",
    "\t\t\t\tfor j in range(GP_AVOUT.shape[1]):\n",
    "\t\t\t\t\tnew_GPR_Class = GPR_Emu(new_nodes_train, new_y_train[:,j], new_yerr_train[:,j], new_nodes_trial)\n",
    "\t\t\t\t\tif len(HPs.shape) == 1: \n",
    "\t\t\t\t\t\t# We do not have individual HPs per bin\n",
    "\t\t\t\t\t\tGP_AVOUT[:,j], GP_STDOUT[:,j], GP_HPs_AllNodes[i,j,:] = new_GPR_Class.GPRsk(HPs, new_yerr_train[:,j], n_restarts_optimizer)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t# We do have individual HPs per bin!\n",
    "\t\t\t\t\t\tGP_AVOUT[:,j], GP_STDOUT[:,j], GP_HPs_AllNodes[i,j,:] = new_GPR_Class.GPRsk(HPs[j], new_yerr_train[:,j], n_restarts_optimizer)\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tnew_GPR_Class = GPR_Emu(new_nodes_train, new_y_train, alpha, new_nodes_trial)\t\t\t\t\n",
    "\t\t\t\tGP_AVOUT, GP_STDOUT, GP_HPs_AllNodes[i,:,:] = new_GPR_Class.GPRsk(HPs, alpha, n_restarts_optimizer) \t\t\t\t# with Scikit-Learn\t\n",
    "\t\t\t\tGP_STDOUT = np.repeat(np.reshape(GP_STDOUT, (-1,1)), GP_AVOUT.shape[1], axis=1)\t\t\t\t# SKL only returns 1 error bar per trial here\n",
    "\t\t\n",
    "\n",
    "\t\t\tif Perform_PCA:\t\t# If Perform_PCA is True, need to convert weights returned from emulator to predictions\n",
    "\t\t\t\tif Include_x:\t\n",
    "\t\t\t\t\tPredictions[i*len(x_coord):(i+1)*len(x_coord),:] = PCACall.Convert_PCAWeights_2_Predictions(GP_AVOUT.transpose(), Train_BFs, Mean_TS).transpose()\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tPredictions[i,:] = PCACall.Convert_PCAWeights_2_Predictions(GP_AVOUT, Train_BFs, Mean_TS)\n",
    "\t\t\telse:\n",
    "\t\t\t\tif Include_x:\n",
    "\t\t\t\t\tPredictions[i*len(x_coord):(i+1)*len(x_coord),:] = GP_AVOUT\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tPredictions[i,:] = GP_AVOUT\n",
    "\n",
    "\t\tt2 = time.time()\n",
    "\t\tprint( \"Whole cross-val. took %.1f s for %i nodes...\" %((t2-t1), NumNodes) )\n",
    "\n",
    "\t\treturn Predictions, GP_HPs_AllNodes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t##### THE FOLLOWING FUNCTIONS ARE ALL USED IN THE MCMC FOR HYPERPARAMS ###################################\n",
    "\n",
    "\t# log prior\n",
    "\tdef lnprior(self, p, lnprior_ranges):\n",
    "\t\tfor i in range(len(p)):\n",
    "\t\t\tif (p[i] < lnprior_ranges[i,0]) or (p[i] > lnprior_ranges[i,1]):\n",
    "\t\t\t\treturn -np.inf\n",
    "\t\treturn 0.\n",
    "\n",
    "\t# log likelihood\n",
    "\tdef lnlike(self, p, y_trial):\n",
    "\n",
    "\t\tL = self.y_train.shape[1]\n",
    "\n",
    "\t\t# New likelihood\n",
    "\t\tkernel = np.exp(p[0])*ExpSquaredKernel(np.exp(p[1:]), ndim=len(p)-1)\n",
    "\t\tgp = george.GP(kernel)\n",
    "\n",
    "\t\tSumLnL = 0.\n",
    "\t\tfor k in range(L):\n",
    "\t\t\tif L == 1:\n",
    "\t\t\t\tgp.compute(self.nodes_train, self.yerr_train)\n",
    "\t\t\telse:\n",
    "\t\t\t\tgp.compute(self.nodes_train, self.yerr_train[:,k])\n",
    "\t\t\tSumLnL += gp.lnlikelihood(self.y_train[:,k])\n",
    "\t\treturn SumLnL\n",
    "\n",
    "\n",
    "\t# log posterior\n",
    "\tdef lnprob(self, p, y_trial, lnprior_ranges):\n",
    "\t\tlp = self.lnprior(p, lnprior_ranges)\n",
    "\t\treturn lp + self.lnlike(p, y_trial) if np.isfinite(lp) else -np.inf\n",
    "\n",
    "\n",
    "\tdef Run_MCMC(self, y_trial, nwalkers, burn_steps, real_steps, lnprior_ranges, p):\n",
    "\n",
    "\t\timport emcee\n",
    "\t\timport time\n",
    "\t\tndim = len(p)\n",
    "\t\tp0 = [p + 1e-2*np.random.randn(ndim) for i in range(nwalkers)]\t\t# starting position\n",
    "\t\tsampler = emcee.EnsembleSampler(nwalkers, ndim, self.lnprob, args=[y_trial, lnprior_ranges])\n",
    "\n",
    "\t\tt0 = time.time()\n",
    "\t\tprint( \"Running burn in of %s steps per walker....\" %burn_steps )\n",
    "\t\tp0,lnp, _ = sampler.run_mcmc(p0, burn_steps)\n",
    "\t\tsampler.reset()\n",
    "\n",
    "\t\tt1 = time.time()\n",
    "\t\tprint( \"First burn-in took %.1f minutes. Running 2nd burn-in...\" %((t1-t0)/60.) )\n",
    "\t\t# set new start point to be a tiny gauss ball around position of whatever walker reached max posterior during burn-in\n",
    "\t\tp = p0[np.argmax(lnp)]\n",
    "\t\tp0 = [p + 1.e-2* p * np.random.randn(ndim) for i in xrange(nwalkers)]\n",
    "\t\tp0, _, _ = sampler.run_mcmc(p0, burn_steps)\n",
    "\t\tsampler.reset()\n",
    "\n",
    "\t\tt2 = time.time()\n",
    "\t\tprint( \"Second burn-in took %.1f minutes. Running the MCMC proper with %s steps per walker\" %(((t2-t1)/60.), real_steps) )\n",
    "\t\tsampler.run_mcmc(p0, real_steps)\n",
    "\t\t# sampler has an attribute called chain that is 3D: nwalkers * real_steps * ndim in dimensionality\n",
    "\t\t# Following line turns it into 2D: (nwalkers*real_steps) * ndim. \n",
    "\t\t# As if there was only one walker.\n",
    "\t\tsamples = sampler.chain[:, :, :].reshape((-1, ndim))\n",
    "\t\tt3 = time.time()\n",
    "\t\tprint( \"Finished. Main MCMC took %.1f minutes. The whole MCMC took %.1f minutes.\" %( ((t3-t2)/60.), ((t3-t0)/60.) ) )\n",
    "\n",
    "\t\treturn samples\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff32d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diagnostic_Plots:\n",
    "\n",
    "\tdef __init__(self, x, y_GP, yerr_GP, y_compare, yerr_compare):\n",
    "\t\tself.x = x\n",
    "\t\tself.y_GP = y_GP\n",
    "\t\tself.yerr_GP = yerr_GP\n",
    "\t\tself.y_compare = y_compare\n",
    "\t\tself.yerr_compare = yerr_compare\n",
    "\n",
    "\tdef errRatio(self, num, errnum, denom, errdenom):\n",
    "\t\tRatio = num / denom -1.\n",
    "\t\terrRatio = Ratio * np.sqrt( (errnum/num)**2. + (errdenom/denom)**2. )\n",
    "\t\treturn Ratio, errRatio\n",
    "\n",
    "\t# Following are diagnostic plots to access accuracy of emulator\n",
    "\tdef Plot_GPvsTrial(self, savename):\n",
    "\t\tFD, errFD = self.errRatio( self.y_GP, self.yerr_GP, self.y_compare, self.yerr_compare ) \n",
    "\t\tplt.figure(figsize=(20,15))\n",
    "        \n",
    "\t\tfor i in range(FD.shape[0]):\n",
    "\t\t\tplt.errorbar( self.x, FD[i,:], yerr=errFD[i,:] )\n",
    "\t\t#plt.plot([self.x.min(),self.x.max()], [0.,0.], 'k--')\n",
    "        \n",
    "\t\tplt.xscale('log')\n",
    "\t\tplt.xlim([self.x.min(), self.x.max()])\n",
    "\t\tplt.ylim([FD.min(), FD.max()])\n",
    "\t\tplt.xlabel(r'$x$')\n",
    "\t\tplt.ylabel(r'(GP - Data) / Data')\n",
    "\t\tplt.savefig(savename)\n",
    "\t\tplt.show()\n",
    "\t\treturn\n",
    "\n",
    "\n",
    "\tdef Plot_CV(self, include_x, savename):\n",
    "\n",
    "\t\tNumNodes = self.y_GP.shape[0]\n",
    "\t\tplt.figure()\n",
    "\t\tfor i in range(NumNodes):\n",
    "\t\t\tif include_x:\n",
    "\t\t\t\tR = (self.y_GP[i*len(self.x):(i+1)*len(self.x),:].transpose() / self.y_compare[i,:])[0,:]\n",
    "\t\t\telse:\n",
    "\t\t\t\tR = self.y_GP[i,:] / self.y_compare[i,:]\n",
    "\t\t\tplt.plot( self.x, R - 1. )\n",
    "\t\tplt.title(r'Cross-Validation: %s Nodes in training set' %NumNodes)\n",
    "\t\tplt.plot([self.x.min(),self.x.max()], [0.,0.], 'k--')\n",
    "\t\tplt.xscale('log')\n",
    "\t\tplt.xlim([self.x.min(), self.x.max()])\n",
    "\t\tplt.ylim([-0.5, 0.5])\n",
    "\t\tplt.xlabel(r'$x$')\n",
    "\t\tplt.ylabel(r'(GP - Data) / Data')\n",
    "\t\tplt.savefig(savename)\n",
    "\t\tplt.show()\n",
    "\t\treturn\n",
    "\n",
    "\n",
    "\tdef Plot_CV_Inacc(self,coords,threshold,labels,limits,savename):\n",
    "\t\timport matplotlib.gridspec as gridspec\n",
    "\t\tif labels == None or len(labels) != coords.shape[1]:\n",
    "\t\t\tnew_labels = []\n",
    "\t\t\tfor i in range(coords.shape[0]):\n",
    "\t\t\t\tnew_labels.append('X%s'%i)\n",
    "\t\t\tlabels = new_labels\n",
    "\n",
    "\t\tstatistic = 100*(self.y_GP / self.y_compare -1)\n",
    "\t\tif len(statistic.shape) > 1:\n",
    "\t\t\tstatistic_avg = np.zeros(statistic.shape[0])\n",
    "\t\t\tfor i in range(len(statistic_avg)):\n",
    "\t\t\t\tstatistic_avg[i] = np.mean( statistic[i,:] )\n",
    "\t\t\tstatistic = statistic_avg\n",
    "\n",
    "\t\tcount_inacc = len(np.where(statistic > threshold)[0])\n",
    "\t\tcmap = plt.get_cmap('jet')\n",
    "\t\tcolors = [cmap(i) for i in np.linspace(0, 1, count_inacc)]\n",
    "\n",
    "\t\tfig = plt.figure(figsize = (12,10)) #figsize = (20,14)\n",
    "\t\tgs1 = gridspec.GridSpec(coords.shape[1]-1,coords.shape[1]-1)\n",
    "\t\tp=0\t# subplot number\n",
    "\t\tfor i in range(coords.shape[1]-1):\n",
    "\t\t\tl=i+1 # which y-axis statistic is plotted on each row.\t\n",
    "\t\t\tfor j in range(coords.shape[1]-1):\n",
    "\t\t\t\tax1 = plt.subplot(gs1[p])\n",
    "\t\t\t\tif j>i:\n",
    "\t\t\t\t\tax1.axis('off')\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tax1.scatter(coords[:,j], coords[:,l], color='dimgrey',s=20)\n",
    "\t\t\t\t\tcolor_idx=0\n",
    "\t\t\t\t\tfor s in range(coords.shape[0]):\n",
    "\t\t\t\t\t\tif statistic[s] > threshold: \n",
    "\t\t\t\t\t\t\tax1.scatter(coords[s,j], coords[s,l], color=colors[color_idx], s=30 )\n",
    "\t\t\t\t\t\t\tcolor_idx+=1\n",
    "\n",
    "\t\t\t\t\t# Decide what the axis limits should be. If limits=None, it doesn't set any.\n",
    "\t\t\t\t\t# If limits is [a,b], limits are set to be a*min and b*max in each dimension.\n",
    "\t\t\t\t\t# Else, limits is interpreted as [ [x1,x2],[y1,y2],[z1,z2]...] for each dimension. \n",
    "\t\t\t\t\tif limits != None:\n",
    "\t\t\t\t\t\tif len(np.array(limits).shape) == 1:\n",
    "\t\t\t\t\t\t\tax1.set_xlim([ limits[0]*coords[:,j].min(),limits[1]*coords[:,j].max() ]) \n",
    "\t\t\t\t\t\t\tax1.set_ylim([ limits[0]*coords[:,l].min(),limits[1]*coords[:,l].max() ]) \n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tax1.set_xlim([ limits[j][0],limits[j][1] ]) \n",
    "\t\t\t\t\t\t\tax1.set_ylim([ limits[l][0],limits[l][1] ])\n",
    "                                                        \n",
    "                                        # If the axes tick labels are too busy, uncomment the following!:\n",
    "\t\t\t\t\t# Set every other x/y-tick to be invisible\n",
    "\t\t\t\t\t#if len(ax1.get_xticklabels()) > 5: # Too many things will plot on x-axis, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# so only plot every third \n",
    "\t\t\t\t\t#\tfor thing in ax1.get_xticklabels():\n",
    "\t\t\t\t\t#\t\tif thing not in ax1.get_xticklabels()[::3]:\n",
    "\t\t\t\t\t#\t\t\tthing.set_visible(False)\n",
    "\t\t\t\t\t#else: \n",
    "\t\t\t\t\t#\tfor thing in ax1.get_xticklabels()[::2]:\n",
    "\t\t\t\t\t#\t\tthing.set_visible(False)\n",
    "\t\t\t\t\t#for thing in ax1.get_yticklabels()[::2]:\n",
    "\t\t\t\t\t#\tthing.set_visible(False)\n",
    "\n",
    "\t\t\t\t\n",
    "\t\t\t\t\t# Get rid of x/y ticks completely for subplots not at the edge\n",
    "\t\t\t\t\tif j==0:\n",
    "\t\t\t\t\t\tax1.set_ylabel(labels[l])\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tax1.set_yticks([])\n",
    "\t\t\t\t\tif i==coords.shape[1]-2:\n",
    "\t\t\t\t\t\tax1.set_xlabel(labels[j])\n",
    "\t\t\t\t\telse:\t\t\t\t\n",
    "\t\t\t\t\t\tax1.set_xticks([])\n",
    "\t\t\t\tp+=1\n",
    "\t\tfig.suptitle(r'Training nodes for which mean accuracy worse than %s per cent (coloured points)' %threshold)\n",
    "\t\tplt.savefig(savename)\n",
    "\t\tplt.show()\n",
    "\t\treturn\n",
    "\n",
    "\n",
    "\n",
    "\tdef Plot_MCMC_Results(self,samples, labels, savename):\n",
    "\t\timport matplotlib.gridspec as gridspec\n",
    "\t\tsteps2plot = np.linspace(0., samples.shape[0]-1, samples.shape[0]) \n",
    "\t\tif labels == None:\n",
    "\t\t\tlabels = [r'$\\ln(a)$']\n",
    "\t\t\tfor i in range(samples.shape[1]-1):\t\t\t\t\n",
    "\t\t\t\tlabels.append('$\\ln(\\rm{HP}_%s)$'%i)\n",
    "\t\tl = len(labels)\n",
    "\n",
    "\t\tfig = plt.figure(figsize = (8,16))\n",
    "\t\tgs1 = gridspec.GridSpec(len(labels), 1)\n",
    "\t\tfor i in range(l):\n",
    "\t\t\t# i = i + 1 # grid spec indexes from 0\n",
    "\t\t\tax1 = plt.subplot(gs1[i])\n",
    "\t\t\tax1.plot(steps2plot, samples[:,i], color='magenta', linewidth=1)\n",
    "\t\t\tif i != (l-1):\n",
    "\t\t\t\tax1.set_xticklabels([])\n",
    "\t\t\tax1.set_ylabel(labels[i])\n",
    "\t\t\tax1.set_xlabel(r'Number of steps')\n",
    "\t\t#gs1.update(wspace=0., hspace=0.) # set the spacing between axes.\n",
    "\t\t#fig.subplots_adjust(hspace=0, wspace=0)\n",
    "\t\tplt.savefig('%s_Steps.png'%savename)\n",
    "\t\t#plt.show()\n",
    "\n",
    "\t\timport corner\n",
    "\t\tfig = corner.corner(samples, labels=labels)\n",
    "\t\tif savename != None and savename != '':\n",
    "\t\t\tplt.savefig('%s_Contours.png' %savename)\n",
    "\n",
    "\t\t# Find the peak HPs\n",
    "\t\tpeak_HPs = []\n",
    "\t\tfor i in range(samples.shape[1]):\n",
    "\t\t\thist, edges = np.histogram( samples[:,i], bins=int(samples.shape[0]/500))\n",
    "\t\t\tdw = 0.5*(edges[1]-edges[0])\n",
    "\t\t\tpeak_HPs.append( float('%.3f'%(edges[np.argmax(hist)]+dw))  )\n",
    "\n",
    "\t\tplt.show()\n",
    "\t\treturn peak_HPs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab1fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
